\documentclass[a4paper,11pt]{article}

\usepackage{graphicx}
\usepackage[utf8x]{inputenc}
\usepackage{fancyvrb}
\usepackage{courier}
\usepackage{helvet}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{pdfpages}
\usepackage[strict]{changepage}
\usepackage{url}

\pdfoptionpdfminorversion=6

\setlength{\voffset}{-1in}
\setlength{\hoffset}{-1in}

\setlength{\topmargin}{2.5cm}		   
\setlength{\headheight}{0cm}		   
\setlength{\headsep}{0cm}		   
\setlength{\oddsidemargin}{3,3cm}
\setlength{\evensidemargin}{2,7cm}
\setlength{\textwidth}{15cm}		   
\setlength{\textheight}{23,5cm}		   
\setlength{\parindent}{0cm}

\newcommand{\emptyLine}{{\LARGE ~\\}}

\begin{document}
	
	\begin{center}
	{\huge\bfseries Predicting Continuous Integration Build Outcome \par}
	{\Large Project Report\par}
	
	\vspace{1cm}
	{\Large\itshape Johannes KÃ¤stle\par}
	{ University of Alberta \par}
	\end{center}


\setlength{\parindent}{0pt}
\setlength{\parskip}{1.5ex plus0.5ex minus0.5ex}

\begin{abstract}
	abstract-text %TODO
\end{abstract}

\section{Introduction}

In modern software engineering using continuous integration is evolved to a best practice. Its goal is it to archive higher productivity while developing and reduce the number of faults and bugs. Thanks to its interweaving with GitHub, TravisCI is commonly used for open source projects. Therefore, there a lots of continuous integration data available. It has be shown that broken builds delay a project significantly \cite{CIDelay}, hence the prediction if a build will fail and why can increase development pace. With this information it is possible to develop tools and techniques which reduce the causes of failing builds. 

In previous researches it was shown that there are correlation between build metrics and outcome \cite{bibid}, and that cascading classifiers work well for predicting the build outcome using  \cite{bibid}. Another research tried to predict build outcome with decision trees also with using data from previous builds. This work wants to try a simpler approach and see if it is possible to predict the build outcome only using the currents build information and its associated commits. For this the TravisTorrent \cite{travisTorrent} dataset analyzed in order to classify the build outcome. For this, three different algorithms, naive bayes, neural networks and decision trees, are used to classify this dataset and it is evaluated if one of them is able to accurately learn this problem. 

\section{Background}

TravisCI is a continuous integration tool to automated compile, build, integrate and test GitHub projects. 

\subsection{Dataset}

For the MSR 2017 challenge the dataset TravisTorrent was created. This study uses the version from January 11, 2017. It contains $3,702,595$ samples. A detailed explanation of all the features can be found on the homepage of TravisTorrent\footnote{\url{https://travistorrent.testroots.org/page_dataformat/}}. Because not all features are useful for this study, the number of features were restricted to some features. A list of features can be found under table \ref{featureTable}. Only builds that were either successful, erroneous or failed were considered. Successful builds were considered as class 1, or true, while the other builds were marked as failed, i. e. class 0, or false. Aborted or canceled builds were ignored, therefor only $2,500,756$ builds were considered in the final dataset. In the final dataset $41.6\%$ of the builds did fail. 

\input{features.tex}

%Size, Features, Where its from stuff

\subsection{Algorithms}

As a baseline an algorithm is used which labels all data to the most frequent class. Then three supervised classification algorithms are tried. 
The first one is Naive Bayes which assumes every feature to be gaussian data. 
The second one is a neural net with one hidden layer and the logistic activation function. As solver for the weight optimization adam\cite{adam}, an optimized stochastic gradient descent, is used. The batchsize is determined automatic, the maximum epochs are set to $200$ and the dataset is shuffle between each epoch.
As third algorithm a decision tree is used. A decision tree classifier is a tree where the sample switches to the left or right child depending on its value for a feature until it is classified in a leaf node. The criterion for splitting is set to the gini impurity, for performance it takes the best random split. 

%Tree, Bayes, MostFrequent, NN

\section{Methodology}

The dataset is stored in a MySQL database. From there all samples are extracted and stored in a matrix. The build outcome is put into a vector $y$ with the value $1$ if the build was successful and $0$ otherwise. The language and previous build feature are stored in a vector; for example the language 'java' is encoded into $[1,0,0]$, while 'javascript' is encoded to $[0,1,0]$. The date features are split up into month, day, daytime in seconds and weekday as integers. 
Since not every sample contains all features, the missing features are set to the mean, so it does not impact the classification.

The actual learning is divided into two parts. First the best parameters for the neural network and the decision tree have to be learned, and afterwards the algorithms need to be compared. Because the dataset is big enough, for the both tasks the dataset is split up in two equal halves. The split and all following splits are done stratified. This complete split is used so that the parameter search does not bias the inter algorithm algorithms. 

%Extraction of Dataset, which features used, missing features

%50:50 Split

\subsection{Parameter}

For looking for the best parameters the first half of the dataset is used for an exhaustive grid search. This cross validation learns a model for every possible combination of the parameters. This is done on a K-Fold with $k=5$. A list of all tried parameters can be found in tables \ref{paramTable} and \ref{paramTable2}. Accuracy is used as scoring method. 


%Which parameters, how checked


\subsection{Cross Algorithms}

The other half of the data is used to compare the different algorithms. For that the dataset is split up into five random sets, where the size of the test set is $10\%$. On this, every of the three and the baseline algorithm is run. For evaluation the confusion matrix is saved for every run. Then the means and standard deviation over the runs is computed for every algorithm.

%Other half of data

\subsection{Statistics}

The algorithms are compared using Welch's t-test, because the variances of the algorithms are different. The baseline algorithm has, as expected, a variance of 0, whereas in test runs the neural networks had generally a high standard deviation. As comparing metrics accuracy, sensitivity and specificity are used, in which the latter ones are equivalent to the AUC of the ROC curve. Those three metrics are used to give a general sense of how good the algorithms are performing and then in detail how good they are at predicting successful and failing builds separately. 

The two tailed t-test is done pairwise for every algorithm with $\mu_0 = \mu_1$ and $\alpha=0.05$ to produce a ranking between the three (plus baseline) algorithm. 

%Which statistics to cross compare

\section{Results}

\subsection{Parameter Findings}

\subsection{Result of Set}

\subsection{Best Algorithm}

\section{Threats / Problems}

\subsection{Not used Parameters}
\subsection{Runtime}
\subsection{Dataset per se}

\section{Conclusion \& Implication}	


\bibliographystyle{ieeetr}
\bibliography{literature}


\end{document}
